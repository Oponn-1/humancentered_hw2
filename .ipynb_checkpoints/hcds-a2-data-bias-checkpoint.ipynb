{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Into Pandas\n",
    "I use pandas to read the 'tab separated' data into a 'data frame' object for ease of interaction.\n",
    "\n",
    "Loading aggression annotation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggression_labels = pd.DataFrame()\n",
    "aggression_labels = pd.read_csv('aggression_annotations.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading aggression comment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggression_comments = pd.DataFrame()\n",
    "aggression_comments = pd.read_csv('aggression_annotated_comments.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading aggression worker data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggression_workers = pd.DataFrame()\n",
    "aggression_workers = pd.read_csv('aggression_worker_demographics.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading toxicity annotation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_labels = pd.DataFrame()\n",
    "toxicity_labels = pd.read_csv('toxicity_annotations.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading toxicity comment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_comments = pd.DataFrame()\n",
    "toxicity_comments = pd.read_csv('toxicity_annotated_comments.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading toxicity worker data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_workers = pd.DataFrame()\n",
    "toxicity_workers = pd.read_csv('toxicity_worker_demographics.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "For both data sets loaded above, I investigate the question of **whether males and females find different words toxic or aggressive**. This would provide some insight into just how much perception of the same content can differ, and therefore what kinds of words (if any) bring some bias into the labelling. If there are particular words for which the labels differ significantly by some demographic breakdown, then any model trained on the data would probably perform poorly when dealing with such words and could apply bias.\n",
    "\n",
    "The intuition motivating this analysis is the thought that there might be certain words or phrases that are more personally applicable to different demographics, so the perception of the negativity of these words will be biased.\n",
    "\n",
    "*First, I must select the annotations that indicate toxicity or aggression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_labels_bad = toxicity_labels.loc[toxicity_labels['toxicity'] == 1]\n",
    "\n",
    "aggression_labels_bad = aggression_labels.loc[aggression_labels['aggression'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Next, I isolate the demographic data I will be focusing on: gender.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "tox_worker_gender = toxicity_workers[['worker_id', 'gender']]\n",
    "\n",
    "agg_worker_gender = aggression_workers[['worker_id', 'gender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now I combine the gender data with the annotation data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "tox_labels_gender = toxicity_labels_bad.merge(tox_worker_gender, on='worker_id')\n",
    "\n",
    "agg_labels_gender = aggression_labels_bad.merge(agg_worker_gender, on='worker_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I also isolate the comment data and combine it with the above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "tox_comments = toxicity_comments[['rev_id', 'comment']]\n",
    "\n",
    "agg_comments = aggression_comments[['rev_id', 'comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "tox = tox_labels_gender.merge(tox_comments, on='rev_id', how='inner')\n",
    "\n",
    "agg = agg_labels_gender.merge(agg_comments, on='rev_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I create lists for all words included in toxic or aggressive comments as annotated by females. I turn the strings to lower case before splitting them into lists of words to avoid some amount of word duplicates.*\n",
    "\n",
    "*Female words for toxicity:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem_tox_words = []\n",
    "fem_tox = tox.loc[tox['gender'] == 'female']\n",
    "fem_tox.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i in range(len(fem_tox)):\n",
    "    fem_tox_words.extend(fem_tox['comment'][i].lower().split())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Female words for aggression:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem_agg_words = []\n",
    "fem_agg = agg.loc[agg['gender'] == 'female']\n",
    "fem_agg.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i in range(len(fem_agg)):\n",
    "    fem_agg_words.extend(fem_agg['comment'][i].lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Male words for toxicity:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "male_tox_words = []\n",
    "male_tox = tox.loc[tox['gender'] == 'male']\n",
    "male_tox.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i in range(len(male_tox)):\n",
    "    male_tox_words.extend(male_tox['comment'][i].lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Male words for aggression:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_agg_words = []\n",
    "male_agg = agg.loc[agg['gender'] == 'male']\n",
    "male_agg.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i in range(len(male_agg)):\n",
    "    male_agg_words.extend(male_agg['comment'][i].lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For reference, I calculate the proportion of male and female entries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male Percentage:  63.36965547455667\n",
      "Female Percentage:  36.60028833565123\n"
     ]
    }
   ],
   "source": [
    "print(\"Male Percentage: \", (len(male_tox) / len(tox)) * 100)\n",
    "print(\"Female Percentage: \", (len(fem_tox) / len(tox)) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using the 'collections' package, I generate a dictionary with counts for each word. I then turn this back into a sorted dataframe to easily examine most prominent words.* \n",
    "\n",
    "*Female word count for toxicity:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem_tox_counts = Counter(fem_tox_words)\n",
    "fem_tox_counts_df = pd.DataFrame.from_dict(fem_tox_counts, orient='index')\n",
    "fem_tox_counts_df.reset_index(inplace=True)\n",
    "fem_tox_counts_df.columns = ['word', 'frequency']\n",
    "fem_tox_counts_df = fem_tox_counts_df.sort_values('frequency', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Female word count for aggression:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem_agg_counts = Counter(fem_agg_words)\n",
    "fem_agg_counts_df = pd.DataFrame.from_dict(fem_agg_counts, orient='index')\n",
    "fem_agg_counts_df.reset_index(inplace=True)\n",
    "fem_agg_counts_df.columns = ['word', 'frequency']\n",
    "fem_agg_counts_df = fem_agg_counts_df.sort_values('frequency', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Male word count for toxicity:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_tox_counts = Counter(male_tox_words)\n",
    "male_tox_counts_df = pd.DataFrame.from_dict(male_tox_counts, orient='index')\n",
    "male_tox_counts_df.reset_index(inplace=True)\n",
    "male_tox_counts_df.columns = ['word', 'frequency']\n",
    "male_tox_counts_df = male_tox_counts_df.sort_values('frequency', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Male word count for aggression:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_agg_counts = Counter(male_agg_words)\n",
    "male_agg_counts_df = pd.DataFrame.from_dict(male_agg_counts, orient='index')\n",
    "male_agg_counts_df.reset_index(inplace=True)\n",
    "male_agg_counts_df.columns = ['word', 'frequency']\n",
    "male_agg_counts_df = male_agg_counts_df.sort_values('frequency', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can observe 40 of the top words featured in toxic comments as labeled by males and females. I skip the top 20 words because in both lists these are just basic parts of speech ('you', 'of', 'is', etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203431</th>\n",
       "      <td>page</td>\n",
       "      <td>8811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203432</th>\n",
       "      <td>would</td>\n",
       "      <td>9039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203433</th>\n",
       "      <td>fucking</td>\n",
       "      <td>9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203434</th>\n",
       "      <td>get</td>\n",
       "      <td>9166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203435</th>\n",
       "      <td>one</td>\n",
       "      <td>9406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203436</th>\n",
       "      <td>his</td>\n",
       "      <td>9490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203437</th>\n",
       "      <td>know</td>\n",
       "      <td>9697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203438</th>\n",
       "      <td>article</td>\n",
       "      <td>9739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203439</th>\n",
       "      <td>nigger</td>\n",
       "      <td>9928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203440</th>\n",
       "      <td>`</td>\n",
       "      <td>10223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203441</th>\n",
       "      <td>has</td>\n",
       "      <td>10484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203442</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>10598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203443</th>\n",
       "      <td>they</td>\n",
       "      <td>11503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203444</th>\n",
       "      <td>am</td>\n",
       "      <td>11882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203445</th>\n",
       "      <td>at</td>\n",
       "      <td>12210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203446</th>\n",
       "      <td>don't</td>\n",
       "      <td>12348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203447</th>\n",
       "      <td>no</td>\n",
       "      <td>12659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203448</th>\n",
       "      <td>who</td>\n",
       "      <td>12694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203449</th>\n",
       "      <td>he</td>\n",
       "      <td>13004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203450</th>\n",
       "      <td>can</td>\n",
       "      <td>13194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203451</th>\n",
       "      <td>just</td>\n",
       "      <td>13320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203452</th>\n",
       "      <td>all</td>\n",
       "      <td>14045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203453</th>\n",
       "      <td>newline_tokennewline_token==</td>\n",
       "      <td>14317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203454</th>\n",
       "      <td>about</td>\n",
       "      <td>14351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203455</th>\n",
       "      <td>from</td>\n",
       "      <td>14574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203456</th>\n",
       "      <td>so</td>\n",
       "      <td>14693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203457</th>\n",
       "      <td>will</td>\n",
       "      <td>14730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203458</th>\n",
       "      <td>by</td>\n",
       "      <td>14934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203459</th>\n",
       "      <td>like</td>\n",
       "      <td>15087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203460</th>\n",
       "      <td>what</td>\n",
       "      <td>15534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203461</th>\n",
       "      <td>or</td>\n",
       "      <td>16990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203462</th>\n",
       "      <td>but</td>\n",
       "      <td>17290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203463</th>\n",
       "      <td>me</td>\n",
       "      <td>17818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203464</th>\n",
       "      <td>if</td>\n",
       "      <td>18065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203465</th>\n",
       "      <td>do</td>\n",
       "      <td>18742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203466</th>\n",
       "      <td>an</td>\n",
       "      <td>19180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203467</th>\n",
       "      <td>was</td>\n",
       "      <td>19558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203468</th>\n",
       "      <td>fuck</td>\n",
       "      <td>19923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203469</th>\n",
       "      <td>with</td>\n",
       "      <td>24589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203470</th>\n",
       "      <td>as</td>\n",
       "      <td>25773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                word  frequency\n",
       "203431                          page       8811\n",
       "203432                         would       9039\n",
       "203433                       fucking       9127\n",
       "203434                           get       9166\n",
       "203435                           one       9406\n",
       "203436                           his       9490\n",
       "203437                          know       9697\n",
       "203438                       article       9739\n",
       "203439                        nigger       9928\n",
       "203440                             `      10223\n",
       "203441                           has      10484\n",
       "203442                     wikipedia      10598\n",
       "203443                          they      11503\n",
       "203444                            am      11882\n",
       "203445                            at      12210\n",
       "203446                         don't      12348\n",
       "203447                            no      12659\n",
       "203448                           who      12694\n",
       "203449                            he      13004\n",
       "203450                           can      13194\n",
       "203451                          just      13320\n",
       "203452                           all      14045\n",
       "203453  newline_tokennewline_token==      14317\n",
       "203454                         about      14351\n",
       "203455                          from      14574\n",
       "203456                            so      14693\n",
       "203457                          will      14730\n",
       "203458                            by      14934\n",
       "203459                          like      15087\n",
       "203460                          what      15534\n",
       "203461                            or      16990\n",
       "203462                           but      17290\n",
       "203463                            me      17818\n",
       "203464                            if      18065\n",
       "203465                            do      18742\n",
       "203466                            an      19180\n",
       "203467                           was      19558\n",
       "203468                          fuck      19923\n",
       "203469                          with      24589\n",
       "203470                            as      25773"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem_tox_counts_df.iloc[len(fem_tox_counts_df) - 60: len(fem_tox_counts_df)-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237608</th>\n",
       "      <td>his</td>\n",
       "      <td>15286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237609</th>\n",
       "      <td>know</td>\n",
       "      <td>15367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237610</th>\n",
       "      <td>shit</td>\n",
       "      <td>15377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237611</th>\n",
       "      <td>nigger</td>\n",
       "      <td>15407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237612</th>\n",
       "      <td>`</td>\n",
       "      <td>15530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237613</th>\n",
       "      <td>fucking</td>\n",
       "      <td>15597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237614</th>\n",
       "      <td>go</td>\n",
       "      <td>15787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237615</th>\n",
       "      <td>has</td>\n",
       "      <td>16102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237616</th>\n",
       "      <td>get</td>\n",
       "      <td>16252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237617</th>\n",
       "      <td>faggot</td>\n",
       "      <td>17321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237618</th>\n",
       "      <td>suck</td>\n",
       "      <td>18290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237619</th>\n",
       "      <td>they</td>\n",
       "      <td>18710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237620</th>\n",
       "      <td>at</td>\n",
       "      <td>19277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237621</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>19442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237622</th>\n",
       "      <td>no</td>\n",
       "      <td>19553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237623</th>\n",
       "      <td>don't</td>\n",
       "      <td>20479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237624</th>\n",
       "      <td>can</td>\n",
       "      <td>20683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237625</th>\n",
       "      <td>who</td>\n",
       "      <td>20905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237626</th>\n",
       "      <td>he</td>\n",
       "      <td>21269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237627</th>\n",
       "      <td>am</td>\n",
       "      <td>21505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237628</th>\n",
       "      <td>just</td>\n",
       "      <td>21739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237629</th>\n",
       "      <td>about</td>\n",
       "      <td>23089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237630</th>\n",
       "      <td>from</td>\n",
       "      <td>23135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237631</th>\n",
       "      <td>so</td>\n",
       "      <td>23818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237632</th>\n",
       "      <td>by</td>\n",
       "      <td>23838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237633</th>\n",
       "      <td>will</td>\n",
       "      <td>24621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237634</th>\n",
       "      <td>all</td>\n",
       "      <td>24711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237635</th>\n",
       "      <td>what</td>\n",
       "      <td>25514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237636</th>\n",
       "      <td>newline_tokennewline_token==</td>\n",
       "      <td>25560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237637</th>\n",
       "      <td>or</td>\n",
       "      <td>26804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237638</th>\n",
       "      <td>but</td>\n",
       "      <td>27348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237639</th>\n",
       "      <td>like</td>\n",
       "      <td>27646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237640</th>\n",
       "      <td>if</td>\n",
       "      <td>28473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237641</th>\n",
       "      <td>me</td>\n",
       "      <td>28998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237642</th>\n",
       "      <td>do</td>\n",
       "      <td>29779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237643</th>\n",
       "      <td>an</td>\n",
       "      <td>30076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237644</th>\n",
       "      <td>was</td>\n",
       "      <td>30471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237645</th>\n",
       "      <td>with</td>\n",
       "      <td>39565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237646</th>\n",
       "      <td>as</td>\n",
       "      <td>39966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237647</th>\n",
       "      <td>fuck</td>\n",
       "      <td>42239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                word  frequency\n",
       "237608                           his      15286\n",
       "237609                          know      15367\n",
       "237610                          shit      15377\n",
       "237611                        nigger      15407\n",
       "237612                             `      15530\n",
       "237613                       fucking      15597\n",
       "237614                            go      15787\n",
       "237615                           has      16102\n",
       "237616                           get      16252\n",
       "237617                        faggot      17321\n",
       "237618                          suck      18290\n",
       "237619                          they      18710\n",
       "237620                            at      19277\n",
       "237621                     wikipedia      19442\n",
       "237622                            no      19553\n",
       "237623                         don't      20479\n",
       "237624                           can      20683\n",
       "237625                           who      20905\n",
       "237626                            he      21269\n",
       "237627                            am      21505\n",
       "237628                          just      21739\n",
       "237629                         about      23089\n",
       "237630                          from      23135\n",
       "237631                            so      23818\n",
       "237632                            by      23838\n",
       "237633                          will      24621\n",
       "237634                           all      24711\n",
       "237635                          what      25514\n",
       "237636  newline_tokennewline_token==      25560\n",
       "237637                            or      26804\n",
       "237638                           but      27348\n",
       "237639                          like      27646\n",
       "237640                            if      28473\n",
       "237641                            me      28998\n",
       "237642                            do      29779\n",
       "237643                            an      30076\n",
       "237644                           was      30471\n",
       "237645                          with      39565\n",
       "237646                            as      39966\n",
       "237647                          fuck      42239"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_tox_counts_df.iloc[len(male_tox_counts_df) - 60: len(male_tox_counts_df)-20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can observe some of the top words featured in aggressive comments as labeled by males and females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177610</th>\n",
       "      <td>which</td>\n",
       "      <td>6674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177611</th>\n",
       "      <td>i'm</td>\n",
       "      <td>6714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177612</th>\n",
       "      <td>up</td>\n",
       "      <td>6735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177613</th>\n",
       "      <td>huge</td>\n",
       "      <td>6754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177614</th>\n",
       "      <td>think</td>\n",
       "      <td>6789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177615</th>\n",
       "      <td>other</td>\n",
       "      <td>6890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177616</th>\n",
       "      <td>why</td>\n",
       "      <td>6890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177617</th>\n",
       "      <td>should</td>\n",
       "      <td>7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177618</th>\n",
       "      <td>there</td>\n",
       "      <td>7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177619</th>\n",
       "      <td>-</td>\n",
       "      <td>7203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177620</th>\n",
       "      <td>any</td>\n",
       "      <td>7229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177621</th>\n",
       "      <td>some</td>\n",
       "      <td>7342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177622</th>\n",
       "      <td>die</td>\n",
       "      <td>7426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177623</th>\n",
       "      <td>been</td>\n",
       "      <td>7496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177624</th>\n",
       "      <td>`</td>\n",
       "      <td>7865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177625</th>\n",
       "      <td>would</td>\n",
       "      <td>7960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177626</th>\n",
       "      <td>people</td>\n",
       "      <td>7982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177627</th>\n",
       "      <td>because</td>\n",
       "      <td>8137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177628</th>\n",
       "      <td>fucking</td>\n",
       "      <td>8241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177629</th>\n",
       "      <td>one</td>\n",
       "      <td>8352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177630</th>\n",
       "      <td>article</td>\n",
       "      <td>8376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177631</th>\n",
       "      <td>get</td>\n",
       "      <td>8503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177632</th>\n",
       "      <td>know</td>\n",
       "      <td>8656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177633</th>\n",
       "      <td>page</td>\n",
       "      <td>8704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177634</th>\n",
       "      <td>his</td>\n",
       "      <td>8772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177635</th>\n",
       "      <td>hate</td>\n",
       "      <td>9376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177636</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>9485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177637</th>\n",
       "      <td>has</td>\n",
       "      <td>9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177638</th>\n",
       "      <td>they</td>\n",
       "      <td>10008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177639</th>\n",
       "      <td>go</td>\n",
       "      <td>10527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177640</th>\n",
       "      <td>at</td>\n",
       "      <td>10897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177641</th>\n",
       "      <td>who</td>\n",
       "      <td>11086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177642</th>\n",
       "      <td>no</td>\n",
       "      <td>11303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177643</th>\n",
       "      <td>don't</td>\n",
       "      <td>11550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177644</th>\n",
       "      <td>can</td>\n",
       "      <td>11810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177645</th>\n",
       "      <td>just</td>\n",
       "      <td>11817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177646</th>\n",
       "      <td>shit</td>\n",
       "      <td>12074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177647</th>\n",
       "      <td>he</td>\n",
       "      <td>12214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177648</th>\n",
       "      <td>newline_tokennewline_token==</td>\n",
       "      <td>12342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177649</th>\n",
       "      <td>from</td>\n",
       "      <td>12497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177650</th>\n",
       "      <td>so</td>\n",
       "      <td>12542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177651</th>\n",
       "      <td>about</td>\n",
       "      <td>12855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177652</th>\n",
       "      <td>faggot</td>\n",
       "      <td>13281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177653</th>\n",
       "      <td>like</td>\n",
       "      <td>13363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177654</th>\n",
       "      <td>am</td>\n",
       "      <td>13382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177655</th>\n",
       "      <td>by</td>\n",
       "      <td>13514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177656</th>\n",
       "      <td>what</td>\n",
       "      <td>14392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177657</th>\n",
       "      <td>or</td>\n",
       "      <td>15194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177658</th>\n",
       "      <td>but</td>\n",
       "      <td>15278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177659</th>\n",
       "      <td>all</td>\n",
       "      <td>15586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177660</th>\n",
       "      <td>will</td>\n",
       "      <td>15794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177661</th>\n",
       "      <td>if</td>\n",
       "      <td>16333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177662</th>\n",
       "      <td>me</td>\n",
       "      <td>16363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177663</th>\n",
       "      <td>do</td>\n",
       "      <td>16548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177664</th>\n",
       "      <td>was</td>\n",
       "      <td>17253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177665</th>\n",
       "      <td>an</td>\n",
       "      <td>17640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177666</th>\n",
       "      <td>suck</td>\n",
       "      <td>18011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177667</th>\n",
       "      <td>with</td>\n",
       "      <td>22209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177668</th>\n",
       "      <td>as</td>\n",
       "      <td>22847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177669</th>\n",
       "      <td>be</td>\n",
       "      <td>24774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                word  frequency\n",
       "177610                         which       6674\n",
       "177611                           i'm       6714\n",
       "177612                            up       6735\n",
       "177613                          huge       6754\n",
       "177614                         think       6789\n",
       "177615                         other       6890\n",
       "177616                           why       6890\n",
       "177617                        should       7000\n",
       "177618                         there       7140\n",
       "177619                             -       7203\n",
       "177620                           any       7229\n",
       "177621                          some       7342\n",
       "177622                           die       7426\n",
       "177623                          been       7496\n",
       "177624                             `       7865\n",
       "177625                         would       7960\n",
       "177626                        people       7982\n",
       "177627                       because       8137\n",
       "177628                       fucking       8241\n",
       "177629                           one       8352\n",
       "177630                       article       8376\n",
       "177631                           get       8503\n",
       "177632                          know       8656\n",
       "177633                          page       8704\n",
       "177634                           his       8772\n",
       "177635                          hate       9376\n",
       "177636                     wikipedia       9485\n",
       "177637                           has       9836\n",
       "177638                          they      10008\n",
       "177639                            go      10527\n",
       "177640                            at      10897\n",
       "177641                           who      11086\n",
       "177642                            no      11303\n",
       "177643                         don't      11550\n",
       "177644                           can      11810\n",
       "177645                          just      11817\n",
       "177646                          shit      12074\n",
       "177647                            he      12214\n",
       "177648  newline_tokennewline_token==      12342\n",
       "177649                          from      12497\n",
       "177650                            so      12542\n",
       "177651                         about      12855\n",
       "177652                        faggot      13281\n",
       "177653                          like      13363\n",
       "177654                            am      13382\n",
       "177655                            by      13514\n",
       "177656                          what      14392\n",
       "177657                            or      15194\n",
       "177658                           but      15278\n",
       "177659                           all      15586\n",
       "177660                          will      15794\n",
       "177661                            if      16333\n",
       "177662                            me      16363\n",
       "177663                            do      16548\n",
       "177664                           was      17253\n",
       "177665                            an      17640\n",
       "177666                          suck      18011\n",
       "177667                          with      22209\n",
       "177668                            as      22847\n",
       "177669                            be      24774"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem_agg_counts_df.iloc[len(fem_agg_counts_df) - 80: len(fem_agg_counts_df)-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202346</th>\n",
       "      <td>any</td>\n",
       "      <td>10483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202347</th>\n",
       "      <td>other</td>\n",
       "      <td>10629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202348</th>\n",
       "      <td>up</td>\n",
       "      <td>10646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202349</th>\n",
       "      <td>gay</td>\n",
       "      <td>10673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202350</th>\n",
       "      <td>why</td>\n",
       "      <td>10812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202351</th>\n",
       "      <td>some</td>\n",
       "      <td>10843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202352</th>\n",
       "      <td>there</td>\n",
       "      <td>10879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202353</th>\n",
       "      <td>should</td>\n",
       "      <td>11118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202354</th>\n",
       "      <td>been</td>\n",
       "      <td>11198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202355</th>\n",
       "      <td>would</td>\n",
       "      <td>11591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202356</th>\n",
       "      <td>fat</td>\n",
       "      <td>11609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202357</th>\n",
       "      <td>`</td>\n",
       "      <td>11794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202358</th>\n",
       "      <td>hate</td>\n",
       "      <td>11972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202359</th>\n",
       "      <td>page</td>\n",
       "      <td>12306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202360</th>\n",
       "      <td>wiki</td>\n",
       "      <td>12322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202361</th>\n",
       "      <td>because</td>\n",
       "      <td>12336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202362</th>\n",
       "      <td>people</td>\n",
       "      <td>12377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202363</th>\n",
       "      <td>ass</td>\n",
       "      <td>12387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202364</th>\n",
       "      <td>fucking</td>\n",
       "      <td>12635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202365</th>\n",
       "      <td>article</td>\n",
       "      <td>12716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202366</th>\n",
       "      <td>one</td>\n",
       "      <td>12733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202367</th>\n",
       "      <td>get</td>\n",
       "      <td>12805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202368</th>\n",
       "      <td>his</td>\n",
       "      <td>13492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202369</th>\n",
       "      <td>know</td>\n",
       "      <td>13567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202370</th>\n",
       "      <td>go</td>\n",
       "      <td>14961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202371</th>\n",
       "      <td>has</td>\n",
       "      <td>15041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202372</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>15278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202373</th>\n",
       "      <td>they</td>\n",
       "      <td>15345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202374</th>\n",
       "      <td>at</td>\n",
       "      <td>16500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202375</th>\n",
       "      <td>fag</td>\n",
       "      <td>16938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202376</th>\n",
       "      <td>no</td>\n",
       "      <td>16959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202377</th>\n",
       "      <td>who</td>\n",
       "      <td>17919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202378</th>\n",
       "      <td>don't</td>\n",
       "      <td>18084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202379</th>\n",
       "      <td>just</td>\n",
       "      <td>18145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202380</th>\n",
       "      <td>can</td>\n",
       "      <td>18925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202381</th>\n",
       "      <td>he</td>\n",
       "      <td>18956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202382</th>\n",
       "      <td>from</td>\n",
       "      <td>19404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202383</th>\n",
       "      <td>so</td>\n",
       "      <td>19620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202384</th>\n",
       "      <td>about</td>\n",
       "      <td>19932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202385</th>\n",
       "      <td>newline_tokennewline_token==</td>\n",
       "      <td>19954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202386</th>\n",
       "      <td>am</td>\n",
       "      <td>20312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202387</th>\n",
       "      <td>all</td>\n",
       "      <td>20562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202388</th>\n",
       "      <td>by</td>\n",
       "      <td>20638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202389</th>\n",
       "      <td>die</td>\n",
       "      <td>20798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202390</th>\n",
       "      <td>suck</td>\n",
       "      <td>20843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202391</th>\n",
       "      <td>what</td>\n",
       "      <td>21511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202392</th>\n",
       "      <td>faggot</td>\n",
       "      <td>22478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202393</th>\n",
       "      <td>nigger</td>\n",
       "      <td>22568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202394</th>\n",
       "      <td>but</td>\n",
       "      <td>22636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202395</th>\n",
       "      <td>or</td>\n",
       "      <td>23017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202396</th>\n",
       "      <td>like</td>\n",
       "      <td>23650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202397</th>\n",
       "      <td>me</td>\n",
       "      <td>25117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202398</th>\n",
       "      <td>an</td>\n",
       "      <td>25514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202399</th>\n",
       "      <td>was</td>\n",
       "      <td>25633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202400</th>\n",
       "      <td>if</td>\n",
       "      <td>25982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202401</th>\n",
       "      <td>do</td>\n",
       "      <td>26031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202402</th>\n",
       "      <td>will</td>\n",
       "      <td>28525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202403</th>\n",
       "      <td>with</td>\n",
       "      <td>34064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202404</th>\n",
       "      <td>as</td>\n",
       "      <td>34170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202405</th>\n",
       "      <td>be</td>\n",
       "      <td>36494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                word  frequency\n",
       "202346                           any      10483\n",
       "202347                         other      10629\n",
       "202348                            up      10646\n",
       "202349                           gay      10673\n",
       "202350                           why      10812\n",
       "202351                          some      10843\n",
       "202352                         there      10879\n",
       "202353                        should      11118\n",
       "202354                          been      11198\n",
       "202355                         would      11591\n",
       "202356                           fat      11609\n",
       "202357                             `      11794\n",
       "202358                          hate      11972\n",
       "202359                          page      12306\n",
       "202360                          wiki      12322\n",
       "202361                       because      12336\n",
       "202362                        people      12377\n",
       "202363                           ass      12387\n",
       "202364                       fucking      12635\n",
       "202365                       article      12716\n",
       "202366                           one      12733\n",
       "202367                           get      12805\n",
       "202368                           his      13492\n",
       "202369                          know      13567\n",
       "202370                            go      14961\n",
       "202371                           has      15041\n",
       "202372                     wikipedia      15278\n",
       "202373                          they      15345\n",
       "202374                            at      16500\n",
       "202375                           fag      16938\n",
       "202376                            no      16959\n",
       "202377                           who      17919\n",
       "202378                         don't      18084\n",
       "202379                          just      18145\n",
       "202380                           can      18925\n",
       "202381                            he      18956\n",
       "202382                          from      19404\n",
       "202383                            so      19620\n",
       "202384                         about      19932\n",
       "202385  newline_tokennewline_token==      19954\n",
       "202386                            am      20312\n",
       "202387                           all      20562\n",
       "202388                            by      20638\n",
       "202389                           die      20798\n",
       "202390                          suck      20843\n",
       "202391                          what      21511\n",
       "202392                        faggot      22478\n",
       "202393                        nigger      22568\n",
       "202394                           but      22636\n",
       "202395                            or      23017\n",
       "202396                          like      23650\n",
       "202397                            me      25117\n",
       "202398                            an      25514\n",
       "202399                           was      25633\n",
       "202400                            if      25982\n",
       "202401                            do      26031\n",
       "202402                          will      28525\n",
       "202403                          with      34064\n",
       "202404                            as      34170\n",
       "202405                            be      36494"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_agg_counts_df.iloc[len(male_agg_counts_df) - 80: len(male_agg_counts_df)-20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxicity: Word Frequency Insights\n",
    "Two main things stand out from the results of word frequency in the toxicity dataset above:\n",
    "1. Males and females reacted similarly to 'nigger' and 'fuck'.\n",
    "2. Males responded to some words that females did not, specifically 'faggot', and 'suck'.\n",
    "\n",
    "The first of these seems to make sense; the n-word is widely understood to be extremely negative, and though 'fuck' is very flexible in its use, it is commonly used to express things such as frustration and anger.\n",
    "\n",
    "The second observation appears to be a solid example of bias in the data. It makes sense that males would respond more strongly to the words 'faggot' and 'suck', as these are both used in homophobic slurs, a staple of toxic masculinity directed from males to each other.\n",
    "\n",
    "This discrepancy **indicates bias** by gender in the annotation of toxicity. Clearly males and females perceive the toxicity of certain words differently based on cultural significance. If annotators respond more strongly to specific words that could offend them personally in their own life, does that mean that the dataset is biased unless the annotators are demographically representative of the Wiki editing platform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggression: Word Frequency Insights\n",
    "Similar to the toxicity results above, there are some words of agreement and some that are unique:\n",
    "1. Males and females agree on 'nigger', 'fuck', 'faggot', 'suck', 'die'\n",
    "2. There are differences in 'shit, 'ass', and 'fat'\n",
    "\n",
    "There is much more agreement here over some of the words that stood out for the toxicity ratings. The toxic masculinity / homophobia associated words are considered aggressive by both males and females, and of course the implication of death.\n",
    "\n",
    "Interestingly, the word 'shit' appears much more in comments labeled aggressive by females. The words 'ass' and 'fat' appear more in those labeled by males. Perhaps due to the cultural way in which 'potty mouth' speech is considered more acceptable or normal for males, the word shit is perceived differently by gender. I have no intuition for why 'ass' and 'fat' would be noticed more by males than females, but it is a difference.\n",
    "\n",
    "Like with the toxicity results, these observations **indicate bias** by gender in the annotation of aggression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Implications\n",
    "Here I consider how the insights from my analysis relate to potential applications as shown in [Perspective Hacks](https://github.com/conversationai/perspectiveapi/wiki/perspective-hacks), what some limitations of using this data might be, and potentially unintended consequences.\n",
    "\n",
    "### Perspective Hacks Applications: Which Would a Model Based on This Data perform Well in?\n",
    "The application that seems most likely to be successful using this data is the 'Toxicity Timeline'. Even though I identified some potential sources of bias in the toxicity annotations, a general view of trends in negative activity can be useful to deal with problem users or group harassment. Since it is not about interpreting specific instances of toxicity, the bias for perception of specific words is less relevant; the words where genders agreed on toxicity were the most frequent.\n",
    "\n",
    "### What Hostile Speech Would be Difficult to Detect?\n",
    "The most difficult speech to deal with, given the Perspective API and this training data, would be sarcasm or otherwise 'ironic' language. The problem with this is that there is no good training data to learn from - we humans ourselves are very inconsistent in recongizing and interpreting these things, especially in writing. The context of a user's previous comments might give indication to whether something is meant literally, but the data used here does not offer that.\n",
    "\n",
    "### What are Unintended Consequences?\n",
    "Two potentially negative consequences that come to mind are: playful language can be restricted, and some demographically specific toxicity could be treated differently, leading to a kind of indirect discrimination. Based on my analyses, there are some words which could easily be identified as toxic or aggressive, but which can also be used in humorous and casual ways ('fuck' and 'shit', for example). Also, I observed that homophobic words appeared more in male labeled comments; if there are other such demographic biases in labelling, perhaps some targeted toxic language will be recognized and dealt with less easily than others, which could leave groups of users in an unfair situation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
